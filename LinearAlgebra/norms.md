## Index
![light](https://user-images.githubusercontent.com/12748752/132402912-1a2a215e-de2f-4536-b28e-e75197136af9.png)
![dark](https://user-images.githubusercontent.com/12748752/132402918-976c6cc7-cc94-4267-9513-b3937504eb63.png)


## Norms in ML
The basic reason why machine learning and many other fields use norms is because we usually use **vectors** or **matrices** as our **basic units of representation**. In fact anything that goes our input or output is usually measured by **vectors** and **matrices**. Norm is the _generalization_ of the notion of **_length_** to **vectors**, **matrices** and **tensors**

#### So there are two basic reasons that we use norms-
1. <ins><b><i>To find out how big or small a particular vector or tensor is </i></b></ins>**:** sometimes we need to estimate the size- for a **scalar** a _weight_ or _pressure_ or _temperature_ there is one single number by which we can get the idea of how big this thing is, whether it is negative or positive the absolute value usually denotes what the size is for a scalar. For a **vector** we have no such single number coz it is a bunch of numbers but suppose you need a single number. So norms sometimes can be thought of as a mapping from a vector or a tensor to a single number or to scalar and actually this is a positive scalar.
2. <ins><b><i>To estimate how close one tensor is to another </i></b></ins>**:** 
   * That is how "big" the difference between two tensors is 
   * **Example**: How close is one image to another?

