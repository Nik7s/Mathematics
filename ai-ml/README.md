## Index
![light](https://user-images.githubusercontent.com/12748752/132402912-1a2a215e-de2f-4536-b28e-e75197136af9.png)
![dark](https://user-images.githubusercontent.com/12748752/132402918-976c6cc7-cc94-4267-9513-b3937504eb63.png)

## Linear Regression
![dark](https://user-images.githubusercontent.com/12748752/132402918-976c6cc7-cc94-4267-9513-b3937504eb63.png)

<img src="https://user-images.githubusercontent.com/12748752/185733992-10bdd718-70b7-4375-a26d-3ee484a8454e.png" width=60%/>

Given some input vector $\large{\color{Purple}\vec{\mathbf{v}}}$, if you want to connect it to some output vector $\large{\color{Purple}\vec{\mathbf{y}}}$ via a linear model, for some reason you think that the connection between input and output, the regression connection is actually through a linearity connection. In that case let us say our $\large{\color{Purple}h(x)}$ with parameters $\large{\color{Purple}w}$ is assumed to be a linear model, in that case all you do is you take a hypothesis function $\large{\color{Purple}h(x)}$, you say that my guess is $\large{\color{Purple}\hat{\mathbf{y}}}$, you will have already got some ground truth $\large{\color{Purple}\hat{\mathbf{y}}}$ and using these two you calculate the cost function $\large{\color{Purple}\mathrm{J}}$ and you feed it back so as to improve w ok by looking at $\huge {\color{Purple} \frac{\partial J }{\partial w}}$ .
