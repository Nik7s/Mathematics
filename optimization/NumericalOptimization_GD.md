## Index
![light](https://user-images.githubusercontent.com/12748752/132402912-1a2a215e-de2f-4536-b28e-e75197136af9.png)
![dark](https://user-images.githubusercontent.com/12748752/132402918-976c6cc7-cc94-4267-9513-b3937504eb63.png)

## Why we need Numerical Optimization?
![dark](https://user-images.githubusercontent.com/12748752/132402918-976c6cc7-cc94-4267-9513-b3937504eb63.png)
<p align="center">
  <img src="https://user-images.githubusercontent.com/12748752/193400409-b4e4ab7a-1795-416f-9514-15f615d02568.png" width=30%/>
</p>

Suppose we have function $\large{\color{Purple} f(x)}$ and let us say $\large{\color{Purple} \vec{x}}$ is a vector and has two components $\large{\color{Purple} x_1}$ and $\large{\color{Purple} x_2}$ . 
* In that case if you knew $\large{\color{Purple} f(x)}$ as an **analytical function** of $\large{\color{Purple} x_1}$ and $\large{\color{Purple} x_2}$, then you could use various ideas such as setting **gradient** of $\large{\color{Purple} f=0}$ and you have standard methodologies to find out what the appropriate **minimum** or **maximum** is.

However, most of the cases what happens is we do not have explicit expressions. So you do not really know what $\large{\color{Purple} f}$ is, so an explicit expression would be something of the sort $\large{\color{Purple}J(w )=ùë§_1^2+ùë§_2^2+ùë§_3^2+4 }$
